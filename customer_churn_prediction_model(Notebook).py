# -*- coding: utf-8 -*-
"""Customer_churn_prediction_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Adityaupadhyay013/Customer-churn-prediction-ml-model/blob/main/Customer_churn_prediction_model.ipynb
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
blastchar_telco_customer_churn_path = kagglehub.dataset_download('blastchar/telco-customer-churn')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

file_path = blastchar_telco_customer_churn_path

import os
print(os.listdir(file_path))

file_path = file_path +"/" + "WA_Fn-UseC_-Telco-Customer-Churn.csv"

df = pd.read_csv(file_path)
df

pip install -U ydata-profiling

from ydata_profiling import ProfileReport
Prof = ProfileReport(df)
Prof.to_file(output_file = "Telecom Cosutomer churn report analysis.html")

"""### gender: OneHotEncoding (Male , Female)
### SeniorCitizen: Encoded(1 , 0)
### Partner: True/False , Dependents , PaperlessBilling
### tenure: Mean	32.371149 (High correlation)
### PhoneService: True/False (High correlation) Imbalanced
### MultipleLines: (High correlation) No/Yes/No phone service
### Internet Service: Categorical (Fiber optic) , DSL , No (High correlation)
### OnlineSecurity: No/Yes/No internet Service (High correlation)
### OnlineBackup: No/Yes/No internet service(High correlation)
### DeviceProtection: No/Yes/No internet service(High correlation)
### TechSupport: No/Yes/No internet service(High correlation)
### SteamingTV: No/Yes/No internet service(High correlation)
### StreamingMovies: No/Yes/No internet service(High correlation)
### Contract: 3 categories , PaymentMethod: 4 categories
### MonthlyCharges: Real number
### TotalCharges: Text
#### Target: Churn: Boolean True/False (73% False , 26% True)

### Is data Linear or non Linear:::
"""

import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as sts

PT = PowerTransformer()
x1 = pd.DataFrame(df[Num_cols])
x_tran = PT.fit_transform(x1)

x_transform = pd.DataFrame(x_tran , columns = Num_cols)

"""### PowerTransformer on TotalCharges. Skewness: -0.1528"""

for cols in x_transform:
  print(f"QQ plot for {cols} column.Skewness value for {cols} is: {x_transform[cols].skew()}")
  sts.probplot(x_transform[cols] , dist = 'norm' , plot = plt)
  plt.show()

for cols in df:
 if df[cols].dtype in ['int32' , 'int64' , 'float32' , 'float64']:
  print(f"QQ plot for {cols} column.Skewness value for {cols} is: {df[cols].skew()}")
  sts.probplot(df[cols] , dist = 'norm' , plot = plt)
  plt.show()

for cols in x_tran:
 if x_tran[cols].dtype in ['int32' , 'int64' , 'float32' , 'float64']:
  print(f"Boxplot for {cols} column")
  sns.boxplot(x_tran[cols])
  plt.show()

"""### I will go with RobustScaler instead of capping::::"""

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'] , errors = 'coerce')

df['TotalCharges']

df['TotalCharges'].isnull().sum()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

cat_encoding = OneHotEncoder()
Ct1 = ColumnTransformer([
    ("Ct" , cat_encoding , Cat_cols)
] , remainder = 'passthrough')

x1 = df.drop('TotalCharges' , axis = 1)
y1 = df['TotalCharges'].isnull().astype(int)

x1_transformed = Ct1.fit_transform(x1)

x1_transformed

Cols = Ct1.get_feature_names_out()

x1_transformed = pd.DataFrame(x1_transformed , columns = Cols)

x1_transformed

x1_transformed.drop('remainder__MonthlyCharges' , axis = 1  , inplace = True)

x1_transformed.drop('remainder__customerID' , axis = 1  , inplace = True)

model = RandomForestClassifier()

score = cross_val_score(model , x1_transformed  , y1 , cv = 5 , scoring = "roc_auc")

print(score)

from sklearn.impute import SimpleImputer

Imp = SimpleImputer(strategy = 'constant' , fill_value = Val)

x1 = df['TotalCharges']

x_transformed = Imp.fit_transform(pd.DataFrame(x1))
x_transformed

x_tran = pd.DataFrame(x_transformed , columns = ['TotalCharges'])

x_tran

"""#### For totalcharges use Simple imputer with mean."""

Val = df['TotalCharges'].mean()+3*df['TotalCharges'].std()

Val

Num_cols = ['tenure' , 'MonthlyCharges' , 'TotalCharges']
Cat_cols = ['gender' , 'SeniorCitizen'  , 'Partner' , 'Dependents' , 'PhoneService' , 'MultipleLines' , 'InternetService' , 'OnlineSecurity' , 'OnlineBackup' ,
            'DeviceProtection' , 'TechSupport' , 'StreamingTV' , 'StreamingMovies' , 'Contract' , 'PaperlessBilling' , 'PaymentMethod']

from sklearn.preprocessing import FunctionTransformer , PowerTransformer , OneHotEncoder , RobustScaler

len(Cat_cols)

# For TotalCharges: Impute missing values then PowerTransform
TotalCharges_pipeline = Pipeline([
    ("Imp" , SimpleImputer(strategy = "mean")),
    ("Norm" , PowerTransformer())
])

# For other numerical columns (tenure, MonthlyCharges): Impute missing values then RobustScale
OtherNum_pipeline = Pipeline([
    ("Imp" , SimpleImputer(strategy = "mean")),
    ("Sscaler" , RobustScaler())
])

# Categorical pipeline remains the same, with added handle_unknown for robustness
Col_Preprocessing = Pipeline([
    ("Encod" , OneHotEncoder(handle_unknown='ignore'))
])

# Redefine the ColumnTransformer to properly handle each set of features
CT = ColumnTransformer([
    ("TotalCharges_proc", TotalCharges_pipeline, ['TotalCharges']),
    ("OtherNum_proc", OtherNum_pipeline, ['tenure', 'MonthlyCharges']),
    ("Col_Preprocessing", Col_Preprocessing, Cat_cols)
], remainder = 'passthrough')

x1 = df.head(50)

x_train

x1

x1_t = CT.fit_transform(x1)

C = CT.get_feature_names_out()

x1_t = pd.DataFrame(x1_t , columns = C)

x1_t

df.drop('customerID' , axis = 1 , inplace = True)

from sklearn.model_selection import train_test_split

X = df.drop('Churn' , axis = 1)
y = df['Churn']

x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.25 , random_state = 42)

x_train.shape

from sklearn.linear_model import LogisticRegression

model = GradientBoostingClassifier()

Pipe = Pipeline([
    ("CT" , CT) ,
    ("LR" , model)
])

x_train

y_train

x_train_trans = CT.fit_transform(x_train)

x_test_trans = CT.transform(x_test)

y_train

for col in Num_cols:
    x_train[col] = pd.to_numeric(x_train[col], errors='coerce')
    x_test[col] = pd.to_numeric(x_test[col], errors='coerce')

# Ensure numerical columns are correctly typed as float in x_train and x_test


Pipe.fit(x_train , y_train)

from sklearn.metrics import classification_report

y_pred = Pipe.predict(x_test)

y_pred

print(classification_report(y_test , y_pred))

pip install xgboost

pip install catboost

pip install LightGBM

pip install optuna

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier , BaggingClassifier , RandomForestClassifier , AdaBoostClassifier , GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import optuna

y

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)



def objective(trial):
  classifier_name = trial.suggest_categorical('classifier' , ['LR' , 'DTC' , 'BGC' , 'RFC' , 'ABC' , 'GBC' , 'GSB' , 'XGB' , 'CBC' , 'LGBM' , 'KNC' , 'SVC'])
  if classifier_name == 'LR':
    model = LogisticRegression()
  if classifier_name == 'DTC':
    model = DecisionTreeClassifier()
  elif classifier_name == 'BGC':
    model = BaggingClassifier()
  elif classifier_name == 'RFC':
    model = RandomForestClassifier()
  elif classifier_name == 'ABC':
    model = AdaBoostClassifier()
  elif classifier_name == 'GBC':
    model = GradientBoostingClassifier()
  elif classifier_name == 'GSB':
    model = GaussianNB()
  elif classifier_name == 'XGB':
    model = XGBClassifier()
  elif classifier_name == 'CBC':
    model = CatBoostClassifier()
  elif classifier_name == 'LGBM':
    model = LGBMClassifier()
  elif classifier_name == 'KNC':
    model = KNeighborsClassifier()
  elif classifier_name == 'SVC':
    model = SVC()
  Pipe = Pipeline([
    ("CT" , CT) ,
    ("model" , model)
  ])
  score = cross_val_score(Pipe , X , y , cv = 5 , scoring = 'f1' , n_jobs = -1)
  return score.mean()

study = optuna.create_study(direction = 'maximize' , sampler = optuna.samplers.TPESampler())
study.optimize(objective , n_trials = 20)

print(study.best_trial.value)
print(study.best_trial.params)

"""### In case of best algorithm selection i got: Accuracy:
### 0.8070435995870702 , i got recall : 0.5334461154678787 with GBC
### {'classifier': 'GBC'} , best precision i got is 0.67284 with GBC

## With LogisticRegression i got: Precision: 0.6777814115210126
### {'classifier': 'LR'}
### accuracy: 0.8076117814052519
### {'classifier': 'LR'}

### With GSB: i got recall: 0.8458961161847143 , precision: 0.67837
### {'classifier': 'GSB'}
### f1: 0.5967011113282349
{'classifier': 'GSB'}

### For more accurate preprocessing i am getting for LogisticRegression:::             precision    recall  f1-score   support

           0       0.84      0.91      0.87      1282
           1       0.69      0.53      0.60       479

    accuracy                           0.81      1761
   macro avg       0.77      0.72      0.74      1761
weighted avg       0.80      0.81      0.80      1761

g:

#### For LR Classification report i got is:               precision    recall  f1-score   support

          No       0.84      0.91      0.87      1282
         Yes       0.69      0.54      0.60       479

    accuracy                           0.81      1761
   macro avg       0.76      0.72      0.74      1761
weighted avg       0.80      0.81      0.80      1761
### For GBC Classification report i got is:
              precision    recall  f1-score   support

          No       0.83      0.91      0.87      1282
         Yes       0.67      0.50      0.57       479

    accuracy                           0.80      1761
   macro avg       0.75      0.70      0.72      1761
weighted avg       0.79      0.80      0.79      1761
### After more accuarte preprocessing:
### i got with GradientBoostingClassifier
              precision    recall  f1-score   support

           0       0.83      0.91      0.87      1282
           1       0.67      0.51      0.58       479

    accuracy                           0.80      1761
   macro avg       0.75      0.71      0.72      1761
weighted avg       0.79      0.80      0.79      1761

### For GSB Classification report i got is:
              precision    recall  f1-score   support

          No       0.93      0.64      0.76      1282
         Yes       0.48      0.86      0.61       479

    accuracy                           0.70      1761
   macro avg       0.70      0.75      0.69      1761
weighted avg       0.80      0.70      0.72      1761
### For more accuarate preprocessing:
### More accurate preprocessing:  
### For GNB:
              precision    recall  f1-score   support

           0       0.93      0.64      0.76      1282
           1       0.47      0.86      0.61       479

    accuracy                           0.70      1761
   macro avg       0.70      0.75      0.68      1761
weighted avg       0.80      0.70      0.72      1761
"""



Num_cols

Num_Preprocessing = Pipeline([
    ("Imp" , SimpleImputer(strategy = "mean")) ,
    ("Norm" , PowerTransformer()) ,
])
Col_Preprocessing = Pipeline([
    ("Encod" , OneHotEncoder())
])

X.isnull().sum()

Tree_CT = ColumnTransformer([
    ("Num_Preprocessing", Num_Preprocessing , Num_cols) ,
    ("Col_Preprocessing" , Col_Preprocessing , Cat_cols)
] , remainder = 'passthrough')

y

y = df['Churn']

y

"""### Now model selection with less data preprocessing"""

def Objective(trial):
  classifier_name = trial.suggest_categorical('classifier' , ['DTC' ,'BGC' , 'RFC' , 'ABC' , 'GBC'])
  if classifier_name == 'DTC':
    model = DecisionTreeClassifier()
  elif classifier_name == 'BGC':
    model = BaggingClassifier()
  elif classifier_name == 'RFC':
    model = RandomForestClassifier()
  elif classifier_name == 'ABC':
    model = AdaBoostClassifier()
  elif classifier_name == 'GBC':
    model = GradientBoostingClassifier()
  Pipe = Pipeline([
      ("Tree_CT" , Tree_CT) ,
      ("model" , model)
  ])
  score = cross_val_score(Pipe , X , y , cv = 5 , n_jobs = -1 , scoring = 'f1')
  return score.mean()

study = optuna.create_study(direction = 'maximize' , sampler = optuna.samplers.TPESampler())
study.optimize(Objective , n_trials = 20)

print(study.best_trial.value)
print(study.best_trial.params)

"""## For tree focused data preprocessing i got:
### Best accuracy: 0.80718554422866
### {'classifier': 'GBC'}

"""







for cols in Cat_cols:
  df[cols] = df[cols].astype("category")

Min_CT = ColumnTransformer([
    ("Ptransform" , PowerTransformer() ,['TotalCharges'])
])

y = le.fit_transform(y)

y

def Objective(trial):
  classifier_name = trial.suggest_categorical('classifier' , ['XGB' , 'CBC' , 'LGBM'])
  if classifier_name == 'XGB':
    model = XGBClassifier(enable_categorical = True)
  elif classifier_name == 'CBC':
    model = CatBoostClassifier()
  elif classifier_name == 'LGBM':
    model = LGBMClassifier()
  Pipe = Pipeline([
      ("Min_CT" , Min_CT) ,
      ("model" , model)
  ])
  score = cross_val_score(Pipe , X , y , cv = 5 , n_jobs = -1 , scoring = 'f1')
  return score.mean()

study = optuna.create_study(direction = 'maximize' , sampler = optuna.samplers.TPESampler())
study.optimize(Objective , n_trials = 20)

print(study.best_trial.value)
print(study.best_trial.params)

"""### Accuracy: 0.7501078698625718
### {'classifier': 'CBC'} , Recall: 0.15836332095597197
{'classifier': 'LGBM'}

### Hyperparameter tuning with GradientBoostingClassifier , Logistic Regression , NaiveBayes Classifier
"""

def Objective(trial):
  classifier_name = trial.suggest_categorical('classifier_name'  ,  ['LR' , 'GBC' , 'GSB'])
  if classifier_name == 'LR':
    C = trial.suggest_float('C' ,0.1 , 5.0 , step = 0.2)
    l1_ratio = trial.suggest_float('l1_ratio' , 0 , 1 , step = 0.2)
    class_weight = trial.suggest_categorical('class_weight' , ['balanced' , None])
    solver = trial.suggest_categorical('solver' , ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'])
    max_iter = trial.suggest_int('max_iter' , 100 , 700 , step = 200)
    model = LogisticRegression(C = C , l1_ratio = l1_ratio , class_weight = class_weight , solver = solver , max_iter = max_iter)
  elif classifier_name == 'GBC':
    loss = trial.suggest_categorical('loss' , ['log_loss', 'exponential'])
    learning_rate = trial.suggest_float('learning_rate' , 0.1 , 5 , step = 0.4)
    n_estimators = trial.suggest_int('n_estimators' , 50 , 500 , step = 50)
    subsample = trial.suggest_float('subsample' , 0.1 , 1.0 , step = 0.2)
    criterion = trial.suggest_categorical('criterion' , ['friedman_mse', 'squared_error'])
    min_samples_split = trial.suggest_int('min_samples_split' , 2 , 10 ,step =  2)
    min_weight_fraction_leaf = trial.suggest_float('min_weight_fraction_leaf' , 0.1 , 0.5 , step = 0.2)
    max_depth = trial.suggest_int('max_depth' , 3 , 200 , step = 20)
    min_impurity_decrease = trial.suggest_int('min_impurity_decrease' , 0.0 , 50 , step = 2)
    max_features = trial.suggest_categorical('max_fearures' , ['sqrt', 'log2'])
    model = GradientBoostingClassifier(loss = loss , learning_rate = learning_rate , n_estimators = n_estimators , subsample = subsample ,
                                       criterion = criterion ,min_samples_split = min_samples_split , min_weight_fraction_leaf = min_weight_fraction_leaf ,
                                       max_depth = max_depth , min_impurity_decrease = min_impurity_decrease , max_features = max_features)
  else:
    model = GaussianNB()
  Pipe = Pipeline([
    ("CT" , CT) ,
    ("model" , model)
  ])
  score = cross_val_score(Pipe , X , y , cv = 5 , scoring = 'f1' , n_jobs = -1)
  return score.mean()

study = optuna.create_study(direction = 'maximize' , sampler = optuna.samplers.TPESampler())
study.optimize(Objective , n_trials = 100)

print(study.best_trial.value)
print(study.best_trial.params)

"""### Best accuracy: 0.8081800640363894  ----------> Model1
{'classifier_name': 'LR', 'C': 0.1, 'l1_ratio': 0.6000000000000001, 'class_weight': None, 'solver': 'newton-cg', 'max_iter': 500}

### Best recall: 0.9106951871657755
{'classifier_name': 'GBC', 'loss': 'log_loss', 'learning_rate': 3.7, 'n_estimators': 50, 'subsample': 0.30000000000000004, 'criterion': 'friedman_mse', 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.30000000000000004, 'max_depth': 43, 'min_impurity_decrease': 24, 'max_fearures': 'sqrt'}


### Again after hyperparameter tuning best recall i get is: 1.0
{'classifier_name': 'GBC', 'loss': 'log_loss', 'learning_rate': 4.1, 'n_estimators': 400, 'subsample': 0.7000000000000001, 'criterion': 'friedman_mse', 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.30000000000000004, 'max_depth': 163, 'min_impurity_decrease': 38, 'max_fearures': 'sqrt'}


### Best precision: 0.681065368152773
{'classifier_name': 'LR', 'C': 0.1, 'l1_ratio': 0.8, 'class_weight': None, 'solver': 'newton-cg', 'max_iter': 100}

### Best f1 i get is: 0.6339126168043113  MODEL5__________________>
{'classifier_name': 'LR', 'C': 3.9000000000000004, 'l1_ratio': 0.0, 'class_weight': 'balanced', 'solver': 'saga', 'max_iter': 700}

"""

model1 = LogisticRegression(C = 3.9000000000000004, l1_ratio = 0.0, class_weight = 'balanced', solver = 'saga', max_iter =  700)

model = GaussianNB()

Pipe = Pipeline([
    ("CT" , CT) ,
    ("model" , model)
  ])

X.drop('customerID' , axis = 1 , inplace = True)

y

df

Pipe.fit(x_train , y_train)

y_pred = Pipe.predict(x_test)

y_pred

print(classification_report(y_test , y_pred))

"""### Model1 Classification report               
                  precision    recall  f1-score   support

           0       0.84      0.91      0.87      1282
           1       0.69      0.53      0.60       479

    accuracy                           0.81      1761
   macro avg       0.76      0.72      0.74      1761
weighted avg       0.80      0.81      0.80      1761


### MODEL5 Classification report
              precision    recall  f1-score   support

           0       0.92      0.75      0.83      1282
           1       0.55      0.82      0.66       479

    accuracy                           0.77      1761
   macro avg       0.74      0.79      0.74      1761
weighted avg       0.82      0.77      0.78      1761

### GaussianNB give me:               
                precision    recall  f1-score   support

           0       0.93      0.64      0.76      1282
           1       0.47      0.86      0.61       479

    accuracy                           0.70      1761
   macro avg       0.70      0.75      0.68      1761
weighted avg       0.80      0.70      0.72      1761

### Untuned VotingClassifier gives me:
              precision    recall  f1-score   support

           0       0.91      0.76      0.83      1282
           1       0.56      0.81      0.66       479

    accuracy                           0.77      1761
   macro avg       0.74      0.78      0.74      1761
weighted avg       0.82      0.77      0.78      1761

### Untuned BaggingClassifier gives me: base_estimator = LogisticRegression
              precision    recall  f1-score   support

           0       0.92      0.75      0.82      1282
           1       0.55      0.82      0.66       479

    accuracy                           0.77      1761
   macro avg       0.73      0.78      0.74      1761
weighted avg       0.82      0.77      0.78      1761

### Untuned BaggingClassifier gives me: base_estimator
GaussianNB      precision    recall  f1-score   support

           0       0.93      0.64      0.76      1282
           1       0.47      0.86      0.61       479

    accuracy                           0.70      1761
   macro avg       0.70      0.75      0.68      1761
weighted avg       0.80      0.70      0.72      1761


### Tuned BaggingClassifier:
              precision    recall  f1-score   support

          No       0.88      0.81      0.84      1282
         Yes       0.58      0.70      0.63       479

    accuracy                           0.78      1761
   macro avg       0.73      0.75      0.74      1761
weighted avg       0.80      0.78      0.79      1761

### Best recall: Hyperparamter tuned VotingClassifier:               
                precision    recall  f1-score   support

          No       0.93      0.64      0.76      1282
         Yes       0.47      0.86      0.61       479

    accuracy                           0.70      1761
   macro avg       0.70      0.75      0.68      1761
weighted avg       0.80      0.70      0.72      1761



### Best precision: Hyperparameter tuned VotingClassifier
              precision    recall  f1-score   support

          No       0.91      0.76      0.83      1282
         Yes       0.56      0.81      0.66       479

    accuracy                           0.77      1761
   macro avg       0.74      0.78      0.74      1761
weighted avg       0.82      0.77      0.78      1761


"""

score = cross_val_score(Pipe , X , y , cv = 5 , scoring = 'f1' , n_jobs = -1)

print(score)
print(score.mean())

model1 = LogisticRegression(C = 3.9000000000000004, l1_ratio = 0.0, class_weight = 'balanced', solver = 'saga', max_iter =  700)

estms = [('tunedLR' , model1) , ('GNB' , GaussianNB())]

from sklearn.ensemble import VotingClassifier , BaggingClassifier

Best_acc = {'classifier_name': 'BGC_NB', 'n_estimators': 165, 'max_features': 5, 'bootstrap_features': True}

model = BaggingClassifier(estimator = GaussianNB() , n_estimators = 165 , max_features = 5 , bootstrap_features = True)

Best_rec = {'classifier_name': 'VTC', 'voting': 'hard', 'weights_1': 3, 'weights_2': 4}

model = VotingClassifier(estimators= estms , voting = 'hard' , weights = [3 , 4])

Best_pre = {'classifier_name': 'VTC', 'voting': 'hard', 'weights_1': 7, 'weights_2': 7}

model = VotingClassifier(estimators = estms , voting = 'hard' , weights = [7 , 7])

Pipe = Pipeline([
    ("CT" , CT) ,
    ("model" , model)
  ])

x_train.drop('customerID' , axis = 1 , inplace = True)
x_test.drop('customerID' , axis = 1 , inplace = True)

Pipe.fit(x_train , y_train)

y_pred = Pipe.predict(x_test)

print(classification_report(y_test , y_pred))

def Objective(trial):
  classifier_name = trial.suggest_categorical('classifier_name' , ['VTC' , 'BGC_LR' , 'BGC_NB'])
  if classifier_name == 'VTC':
    voting = trial.suggest_categorical('voting' , ['hard' , 'soft'])
    weights_1 = trial.suggest_int('weights_1' , 1 , 10 , step = 1)
    weights_2 = trial.suggest_int('weights_2' , 1 , 10 , step = 1)
    model = VotingClassifier(estimators= estms , voting = voting , weights = (weights_1 , weights_2))
  elif classifier_name == 'BGC_LR' or 'BGC_NB':
    if classifier_name == 'BGC_LR':
      estimator = model1
    else:
      estimator = GaussianNB()
    n_estimators = trial.suggest_int('n_estimators' , 5 , 200 , step = 20)
    max_features = trial.suggest_categorical('max_features' , [0.5 ,1 ,  5 , 10 , 15 , 20])
    bootstrap_features = trial.suggest_categorical('bootstrap_features' , [True , False])
    if bootstrap_features == True:
      oob_score = True
    else:
      oob_score = False
    model = BaggingClassifier(estimator= estimator , n_estimators = n_estimators , max_features = max_features , bootstrap_features = bootstrap_features ,
                              oob_score = oob_score)
  Pipe = Pipeline([
    ("CT" , CT) ,
    ("model" , model)
  ])
  score = cross_val_score(Pipe , X , y , cv = 5 , scoring = 'f1' , n_jobs = -1)
  return score.mean()

study = optuna.create_study(direction = 'maximize',sampler = optuna.samplers.TPESampler())
study.optimize(Objective , n_trials = 50)

print(study.best_trial.value)
print(study.best_trial.params)

"""### Best accuracy (Ensemble algorithms hyperparameter tuning):
0.7741015549390283
{'classifier_name': 'BGC_NB', 'n_estimators': 165, 'max_features': 5, 'bootstrap_features': True}


### Best recall: 0.844825163796935
{'classifier_name': 'VTC', 'voting': 'hard', 'weights_1': 3, 'weights_2': 4}


### Best precision: 0.5348506339573349
{'classifier_name': 'VTC', 'voting': 'hard', 'weights_1': 7, 'weights_2': 7}

"""

import joblib

from google.colab import drive
drive.mount('/content/drive')

joblib.dump(Pipe , '/content/drive/MyDrive/Customer Churn Prediction model(TLR).joblib')

joblib.dump(Pipe , '/content/drive/MyDrive/Customer Churn Prediction model(GNB).joblib')

model = joblib.load("/content/drive/MyDrive/Customer Churn Prediction model(GNB).joblib")

y_pred = model.predict(x_test)

for items , items1 in zip(y_pred , y_test):
  print("Actual: " , items1 , "Predicted: " , items)

"""### Task: Hyperparameter tuning in ensemble models + comparison of current saved models.

### I am finalizing Gaussian Naive Bayes ml algorithm for my model.
"""

x_test

df['PhoneService'].category()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'] , errors = 'coerce')

df['TotalCharges'].describe()

df

df[df['InternetService'] == 'No']['Contract'].value_counts()





